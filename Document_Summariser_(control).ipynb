{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document Summariser (control)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1E09FEzEIXAAnEZRgvqs09ueSPlOoOhoY",
      "authorship_tag": "ABX9TyPyoIa18C5CyscUSUpNqq/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrFlygerian/NLP-Document-Summary/blob/master/Document_Summariser_(control).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7PyeRFmiHe",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation with LSTM Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00EoEDm3TpT6",
        "colab_type": "text"
      },
      "source": [
        "#### First things first\n",
        "\n",
        "To prepare the notebook, google drive must be mounted and the directory with the relevant files (weights, modules, data etc) must be navigated to. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOTh2wN3JehQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMfKMZ_XKiZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks\"\n",
        "%cd \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9FPKr53S_rt",
        "colab_type": "text"
      },
      "source": [
        "## The Project Aims\n",
        "\n",
        "\n",
        "*   To create a contextual summary of a given document automatically\n",
        "*   To compare abilities of deep learning on control and real world data\n",
        "*   Understand deep learning's abilities and limitations\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDjdnc9VVVcg",
        "colab_type": "text"
      },
      "source": [
        "#### The Project Ingredients\n",
        "\n",
        "\n",
        "*   A set of control data (the well known and used nltk corpus for Alice in Wonderland was chosen)\n",
        "\n",
        "*   A set of 'real world' data (some essays on a given topic were used for this experiment, not shown in this notebook)\n",
        "*   The Spyder IDE, numpy, system modules (later transferred to Google Collab)\n",
        "*   Keras and related modules\n",
        "* A decent laptop (16GB RAM, RYZEN 7 CPU, RADEON VEGA GPU)\n",
        "\n",
        "The relevant libraries and control data are imported below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meJdiTlYlnan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#data manipulation\n",
        "import numpy\n",
        "import sys\n",
        "\n",
        "#keras modules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWeHb1aqmFCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def txtfile2txt(textfile):\n",
        "    \n",
        "    raw_text = open(textfile, 'r', encoding = 'utf-8')\n",
        "    raw_text = raw_text.read()\n",
        "    raw_text = raw_text.lower()\n",
        "    \n",
        "    return raw_text\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haYN9chbnAbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_file = \"wonderland.txt\"\n",
        "raw_text = txtfile2txt(text_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MEhXsrT3sLy",
        "colab_type": "text"
      },
      "source": [
        "## The Project Method\n",
        "\n",
        "\n",
        "1. Load files and extract text\n",
        "2. Map each unique character in text to a number and store in a dictionary\n",
        "2. Create a 'moving window' of arbitrary length to generate sequences of characters (mapped to numbers) as inputs \n",
        "4. Use the letter immmediately following the sequence in the text as output\n",
        "5. Store inputs and outputs and reformat them seperately for use with Keras\n",
        "6. Define model (add your bells and whistles), create checkpoints and fit model to reformatted data\n",
        "7. Test model by feeding it with a random sequence from the input data\n",
        "8. Model predicts characters which should follow, and these characters are joined together to form a sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ8kpAWvnDj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhJes6gxnevN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbB5_GRAngiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5poKLGwinjre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.load_weights('starting-weight.hdf5')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP-cZuTzoA_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define checkpoint\n",
        "filepath=\"control-weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhR1GTGUqd2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "71f6c85f-3d7c-4ba8-b684-18f578995164"
      },
      "source": [
        "%%time\n",
        "#fit model\n",
        "#model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "\n",
        "model.fit(X, y, epochs=4, batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "163681/163681 [==============================] - 202s 1ms/step - loss: 1.8128\n",
            "Epoch 2/4\n",
            "163681/163681 [==============================] - 193s 1ms/step - loss: 1.8061\n",
            "Epoch 3/4\n",
            "163681/163681 [==============================] - 189s 1ms/step - loss: 1.7974\n",
            "Epoch 4/4\n",
            "163681/163681 [==============================] - 185s 1ms/step - loss: 1.7827\n",
            "CPU times: user 18min 12s, sys: 2min 37s, total: 20min 50s\n",
            "Wall time: 12min 49s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fc6005cc908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCycHdzz5aEM",
        "colab_type": "text"
      },
      "source": [
        "## The Project Experiment\n",
        "\n",
        "\n",
        "1.   Ran model(s) over multiple epochs (between 15 and 50)\n",
        "2.   Used multiple weights in prediction to test effect of loss reduction\n",
        "3. Compared results on control and real world data\n",
        "4. Compared runtimes on local machine and later using Google Collab\n",
        "5. Attempted to get a handle on version control\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jzqNzmxqgPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "2faee978-ec7f-4a29-c425-d3573f774936"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" a great\n",
            "hurry, muttering to himself as he came, 'oh! the duchess, the duchess!\n",
            "oh! won't she be sava \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umn9XiiqtY_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d2ff034e-144d-4a67-a814-6c63f00a923c"
      },
      "source": [
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ng ' the saod this as the could sot she way at the could, \n",
            "'the care rinng then the marter of the bareerin,' said the kanter. 'io would you well to mo ' \n",
            "the march hare war toen a lomtte or two of the care was toink agaone the had geve a dang wand the tam to sar to tee thet sae tas aoing to thet the was anwing toted her hand and the was aolnnld the had geve and the tam ao all  and her seat soene in the was aol a ling of ges ann aoutoen ana and coe aoutoeng, and she was aowinuid to tay toee a ler\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-o1DWqJ6hK2",
        "colab_type": "text"
      },
      "source": [
        "## The project results\n",
        "\n",
        "\n",
        "*   So far, not that good!\n",
        "\n",
        "*   Control data generates seperate words, but not clear english, little sense made, and is prone to repetition after +500 characters are generated \n",
        "*   Seed text from real word data creates individual words for the first ~100 characters and then starts to repeat characters\n",
        "*Repetitive nature seen in both samples for early epochs and losses of $1<x<2$ ($2<$ losses are next to useless)\n",
        "* At ‘very low’ losses ($1<x<1.1$) some sentences and structure appear for both texts\n",
        "\n",
        "* It’s not easy to explain the predictions/results (DL networks are a black box by construction)\n",
        "* Runtime in Google Collab is significantly less \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOkQr1fJ8WZl",
        "colab_type": "text"
      },
      "source": [
        "## The Project Conclusion\n",
        "* The heavy matrix calculus nature makes DL networks very powerful and easy conceptually to understand, as well as very difficult to break down for insight into specific predictions and problem domains\n",
        "* Very computer intensive\n",
        "* Run time is in the order of hours/days\n",
        "* Keras allows for an iterative process (can save weights, which are core components for the model, and reload them for improvement)\n",
        "* Using Google GPUs via Collab speed up runtime dramatically, so it’s probably best to use those services for future DL endeavours\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B40DyEUH8qRV",
        "colab_type": "text"
      },
      "source": [
        "## Future Questions\n",
        "* Can I used weights trained from one text on another text?\n",
        "* Can I improve the run time and quality simultaneously?\n",
        "* How can I evaluate how well my programme did (both quantitively and pictorially)\n",
        "* Can I generalise weights for text (and potentially other problems)?\n",
        "* Making decent DL models takes a lot of time and resources. Are there pretrained, adaptable models available (ideally for either free or very cheap) that can do the jobs I’m trying to do?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYb98xSyED_R",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1E09FEzEIXAAnEZRgvqs09ueSPlOoOhoY#scrollTo=UYb98xSyED_R)"
      ]
    }
  ]
}

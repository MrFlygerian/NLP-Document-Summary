{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document Summariser (control)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1E09FEzEIXAAnEZRgvqs09ueSPlOoOhoY",
      "authorship_tag": "ABX9TyOfomoOzaQsSh785pKz3bz8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrFlygerian/NLP-Document-Summary/blob/master/Document_Summariser_(control).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7PyeRFmiHe",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation with LSTM Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00EoEDm3TpT6",
        "colab_type": "text"
      },
      "source": [
        "#### First things first\n",
        "\n",
        "To prepare the notebook, google drive must be mounted and the directory with the relevant files (weights, modules, data etc) must be navigated to. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOTh2wN3JehQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "502d51a4-ce0c-4865-ddf9-0d6d8cc30e88"
      },
      "source": [
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMfKMZ_XKiZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "0317ba61-4915-434c-8223-d56369817b2c"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Document summariser\"\n",
        "%cd \"/content/drive/My Drive/Document summariser\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " control-starting-weight.hdf5\t\t   __pycache__\n",
            "'Document Summariser (control).ipynb'\t   README.md\n",
            "'Document summariser (experiment).ipynb'   Untitled\n",
            " experiment-starting-weight.hdf5\t   wonderland.txt\n",
            " get_docx_text.py\n",
            "/content/drive/My Drive/Document summariser\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9FPKr53S_rt",
        "colab_type": "text"
      },
      "source": [
        "## The Project Aims\n",
        "\n",
        "\n",
        "*   To create a contextual summary of a given document automatically\n",
        "*   To compare abilities of deep learning on control and real world data\n",
        "*   Understand deep learning's abilities and limitations\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDjdnc9VVVcg",
        "colab_type": "text"
      },
      "source": [
        "#### The Project Ingredients\n",
        "\n",
        "\n",
        "*   A set of control data (the well known and used nltk corpus for Alice in Wonderland was chosen)\n",
        "\n",
        "*   A set of 'real world' data (some essays on a given topic were used for this experiment, not shown in this notebook)\n",
        "*   The Spyder IDE, numpy, system modules (later transferred to Google Collab)\n",
        "*   Keras and related modules\n",
        "* A decent laptop (16GB RAM, RYZEN 7 CPU, RADEON VEGA GPU)\n",
        "\n",
        "The relevant libraries and control data are imported below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meJdiTlYlnan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "97852eac-9fea-4e32-9f53-281f4c2f000e"
      },
      "source": [
        "%%time\n",
        "#data manipulation\n",
        "import numpy\n",
        "import sys\n",
        "\n",
        "#keras modules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 42 µs, sys: 9 µs, total: 51 µs\n",
            "Wall time: 52.7 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWeHb1aqmFCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def txtfile2txt(textfile):\n",
        "    \n",
        "    raw_text = open(textfile, 'r', encoding = 'utf-8')\n",
        "    raw_text = raw_text.read()\n",
        "    raw_text = raw_text.lower()\n",
        "    \n",
        "    return raw_text\n",
        "    "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haYN9chbnAbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_file = \"wonderland.txt\"\n",
        "raw_text = txtfile2txt(text_file)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MEhXsrT3sLy",
        "colab_type": "text"
      },
      "source": [
        "## The Project Method\n",
        "\n",
        "\n",
        "1. Load files and extract text\n",
        "2. Map each unique character in text to a number and store in a dictionary\n",
        "2. Create a 'moving window' of arbitrary length to generate sequences of characters (mapped to numbers) as inputs \n",
        "4. Use the letter immmediately following the sequence in the text as output\n",
        "5. Store inputs and outputs and reformat them seperately for use with Keras\n",
        "6. Define model (add your bells and whistles), create checkpoints and fit model to reformatted data\n",
        "7. Test model by feeding it with a random sequence from the input data\n",
        "8. Model predicts characters which should follow, and these characters are joined together to form a sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ8kpAWvnDj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhJes6gxnevN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1162e307-5f93-462b-8b4d-1c8e2e616ce2"
      },
      "source": [
        "%%time\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.42 s, sys: 275 ms, total: 1.69 s\n",
            "Wall time: 1.69 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbB5_GRAngiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5poKLGwinjre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.load_weights('control-starting-weight.hdf5')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP-cZuTzoA_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define checkpoint\n",
        "filepath=\"control-weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhR1GTGUqd2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "17270777-2512-49b2-c0e2-a006963c4b9d"
      },
      "source": [
        "%%time\n",
        "#fit model\n",
        "#model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "\n",
        "#model.fit(X, y, epochs=4, batch_size=128)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 6.68 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCycHdzz5aEM",
        "colab_type": "text"
      },
      "source": [
        "## The Project Experiment\n",
        "\n",
        "\n",
        "1.   Ran model(s) over multiple epochs (between 15 and 50)\n",
        "2.   Used multiple weights in prediction to test effect of loss reduction\n",
        "3. Compared results on control and real world data\n",
        "4. Compared runtimes on local machine and later using Google Collab\n",
        "5. Attempted to get a handle on version control\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jzqNzmxqgPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "c0d6e8c6-2916-4e63-dc1a-ffd1d59245f9"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  go\n",
            "after that savage queen: so she waited.\n",
            "\n",
            "the gryphon sat up and rubbed its eyes: then it watched \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umn9XiiqtY_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "98a0690f-c37a-4a85-f9b9-25f3535b04dd"
      },
      "source": [
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " the hirt and then she was sot oo tote to tee thet sad thing whs hav bever her. \n",
            "'ie toene io the mabb of the sort,' she gatter weit on, 'in you dnn't enow in the wait to the gorte de io she wase toin in the garden. whu hn e lavee waid to the toins oo thetg that io the whil woin iade to teel to tey to tey along the taise oh the coerte of the coerte, and soond to her eere and thete whet sare than it was a little so twon that she was a little so twt of the wabbet had and crundrs that she was a lit\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-o1DWqJ6hK2",
        "colab_type": "text"
      },
      "source": [
        "## The project results\n",
        "\n",
        "\n",
        "*   So far, not that good!\n",
        "\n",
        "*   Control data generates seperate words, but not clear english, little sense made, and is prone to repetition after +500 characters are generated \n",
        "*   Seed text from real word data creates individual words for the first ~100 characters and then starts to repeat characters\n",
        "*Repetitive nature seen in both samples for early epochs and losses of $1<x<2$ ($2<$ losses are next to useless)\n",
        "* At ‘very low’ losses ($1<x<1.1$) some sentences and structure appear for both texts\n",
        "\n",
        "* It’s not easy to explain the predictions/results (DL networks are a black box by construction)\n",
        "* Runtime in Google Collab is significantly less \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOkQr1fJ8WZl",
        "colab_type": "text"
      },
      "source": [
        "## The Project Conclusion\n",
        "* The heavy matrix calculus nature makes DL networks very powerful and easy conceptually to understand, as well as very difficult to break down for insight into specific predictions and problem domains\n",
        "* Very computer intensive\n",
        "* Run time is in the order of hours/days\n",
        "* Keras allows for an iterative process (can save weights, which are core components for the model, and reload them for improvement)\n",
        "* Using Google GPUs via Collab speed up runtime dramatically, so it’s probably best to use those services for future DL endeavours\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B40DyEUH8qRV",
        "colab_type": "text"
      },
      "source": [
        "## Future Questions\n",
        "* Can I used weights trained from one text on another text?\n",
        "* Can I improve the run time and quality simultaneously?\n",
        "* How can I evaluate how well my programme did (both quantitively and pictorially)\n",
        "* Can I generalise weights for text (and potentially other problems)?\n",
        "* Making decent DL models takes a lot of time and resources. Are there pretrained, adaptable models available (ideally for either free or very cheap) that can do the jobs I’m trying to do?\n",
        "\n"
      ]
    }
  ]
}